{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12145206,"sourceType":"datasetVersion","datasetId":7649233}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Attention (MAT)","metadata":{}},{"cell_type":"code","source":"# Import Pustaka Standar dan PyTorch\nimport os\nimport re\nimport math\nimport collections\nfrom functools import partial\nimport logging\nimport random\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import model_zoo\nimport numpy as np\nimport kornia # Diperlukan untuk AGDA\n\n# Pengaturan logging dasar\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Kornia version: {kornia.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:26.797698Z","iopub.execute_input":"2025-06-13T01:12:26.798183Z","iopub.status.idle":"2025-06-13T01:12:36.265795Z","shell.execute_reply.started":"2025-06-13T01:12:26.798160Z","shell.execute_reply":"2025-06-13T01:12:36.264986Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nKornia version: 0.8.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Fungsi Utilitas dan Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Parameter Global dan Utilitas Dasar","metadata":{}},{"cell_type":"code","source":"# Parameter Global dan Utilitas Dasar\n# Parameters for the entire model (stem, all blocks, and head)\nGlobalParams = collections.namedtuple('GlobalParams', [\n    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n    'num_classes', 'width_coefficient', 'depth_coefficient',\n    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple('BlockArgs', [\n    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nclass SwishImplementation(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.sigmoid(i)\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = torch.sigmoid(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass MemoryEfficientSwish(nn.Module):\n    def forward(self, x):\n        return SwishImplementation.apply(x)\n\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    \"\"\" Drop connect. \"\"\"\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\nclass Identity(nn.Module):\n    def __init__(self, ):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.267075Z","iopub.execute_input":"2025-06-13T01:12:36.267685Z","iopub.status.idle":"2025-06-13T01:12:36.277556Z","shell.execute_reply.started":"2025-06-13T01:12:36.267663Z","shell.execute_reply":"2025-06-13T01:12:36.276804Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Utilitas Konvolusi dengan Padding","metadata":{}},{"cell_type":"code","source":"# Utilitas Konvolusi dengan Padding\ndef get_same_padding_conv2d(image_size=None):\n    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n        Static padding is necessary for ONNX exporting of models. \"\"\"\n    if image_size is None:\n        return Conv2dDynamicSamePadding\n    else:\n        return partial(Conv2dStaticSamePadding, image_size=image_size)\n\n\nclass Conv2dDynamicSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dStaticSamePadding(nn.Conv2d):\n    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n        assert image_size is not None\n        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n        else:\n            self.static_padding = Identity()\n\n    def forward(self, x):\n        x = self.static_padding(x)\n        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.278327Z","iopub.execute_input":"2025-06-13T01:12:36.278580Z","iopub.status.idle":"2025-06-13T01:12:36.312001Z","shell.execute_reply.started":"2025-06-13T01:12:36.278558Z","shell.execute_reply":"2025-06-13T01:12:36.311371Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Utilitas untuk Parameter dan Konfigurasi Backbone","metadata":{}},{"cell_type":"code","source":"#Utilitas untuk Parameter dan Konfigurasi Backbone\nclass BlockDecoder(object):\n    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n    @staticmethod\n    def _decode_block_string(block_string):\n        assert isinstance(block_string, str)\n        ops = block_string.split('_')\n        options = {}\n        for op in ops:\n            splits = re.split(r'(\\d.*)', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n        assert (('s' in options and len(options['s']) == 1) or\n                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options['k']),\n            num_repeat=int(options['r']),\n            input_filters=int(options['i']),\n            output_filters=int(options['o']),\n            expand_ratio=int(options['e']),\n            id_skip=('noskip' not in block_string),\n            se_ratio=float(options['se']) if 'se' in options else None,\n            stride=[int(options['s'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        \"\"\"Encodes a block to a string.\"\"\"\n        args = [\n            'r%d' % block.num_repeat,\n            'k%d' % block.kernel_size,\n            's%d%d' % (block.stride[0], block.stride[0]),\n            'e%s' % block.expand_ratio,\n            'i%d' % block.input_filters,\n            'o%d' % block.output_filters\n        ]\n        if hasattr(block, 'se_ratio') and block.se_ratio is not None and 0 < block.se_ratio <= 1:\n            args.append('se%s' % block.se_ratio)\n        if hasattr(block, 'id_skip') and block.id_skip is False:\n            args.append('noskip')\n        return '_'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\nurl_map = {\n    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c321c.pth',\n    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth',\n    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth',\n    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth',\n    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth',\n    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth',\n    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth',\n    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth',\n}\n\nurl_map_advprop = {\n    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b0-b64d5a18.pth',\n    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b1-0f3ce85a.pth',\n    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b2-6e9d97e5.pth',\n    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b3-cdd7c0f4.pth',\n    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b4-44fb3a87.pth',\n    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b5-86493f6b.pth',\n    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b6-ac80338e.pth',\n    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b7-4652b6dd.pth',\n    'efficientnet-b8': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b8-22a8fe65.pth',\n}\n\ndef load_pretrained_weights(model, model_name, load_fc=True, advprop=False):\n    \"\"\" Loads pretrained weights, and optionally pinpoints error on incorrect architecture. \"\"\"\n    urls = url_map_advprop if advprop else url_map\n    if model_name not in urls:\n        raise ValueError(f'model_name {model_name} not in pretrained weights')\n\n    state_dict = model_zoo.load_url(urls[model_name], progress=True, map_location='cpu')\n\n    if not load_fc:\n        # Hapus bobot dari layer klasifikasi (fully connected) jika num_classes berbeda\n        state_dict.pop('_fc.weight')\n        state_dict.pop('_fc.bias')\n\n    # Muat state_dict ke model\n    ret = model.load_state_dict(state_dict, strict=False)\n    \n    # Periksa apakah ada layer yang tidak cocok\n    assert set(ret.missing_keys) == set(), f'Missing keys: {ret.missing_keys}'\n    logging.info(f\"Loaded pretrained weights for {model_name}\")\n\n# >> LETAKKAN SEBELUM `class EfficientNet(nn.Module):` <<\n\ndef efficientnet_params(model_name):\n    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n        'efficientnet-b8': (2.2, 3.6, 672, 0.5),\n        'efficientnet-l2': (4.3, 5.3, 800, 0.5),\n    }\n    return params_dict[model_name]\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n    \"\"\" Creates a efficientnet model. \"\"\"\n    blocks_args_str = [\n        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n        'r1_k3_s11_e6_i192_o320_se0.25',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args_str)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99, batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate, drop_connect_rate=drop_connect_rate,\n        num_classes=num_classes, width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient, depth_divisor=8,\n        min_depth=None, image_size=image_size,\n    )\n    return blocks_args, global_params\n\ndef get_model_params(model_name, override_params):\n    \"\"\" Get the block args and global params for a given model \"\"\"\n    if model_name.startswith('efficientnet'):\n        w, d, s, p = efficientnet_params(model_name)\n        blocks_args, global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n    else:\n        raise NotImplementedError(f'model name is not pre-defined: {model_name}')\n    if override_params:\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.313944Z","iopub.execute_input":"2025-06-13T01:12:36.314534Z","iopub.status.idle":"2025-06-13T01:12:36.331367Z","shell.execute_reply.started":"2025-06-13T01:12:36.314509Z","shell.execute_reply":"2025-06-13T01:12:36.330670Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Arsitektur Backbone","metadata":{}},{"cell_type":"markdown","source":"## Arsitektur EfficientNet","metadata":{}},{"cell_type":"markdown","source":"### Blok Penyusun EfficientNet","metadata":{}},{"cell_type":"code","source":"# Blok Penyusun EfficientNet\nclass MBConvBlock(nn.Module):\n    \"\"\" Mobile Inverted Residual Bottleneck Block \"\"\"\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip\n\n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        inp = self._block_args.input_filters\n        oup = self._block_args.input_filters * self._block_args.expand_ratio\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2d(\n            in_channels=oup, out_channels=oup, groups=oup,\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n        self._swish = MemoryEfficientSwish()\n\n    def forward(self, inputs, drop_connect_rate=None):\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._swish(self._bn0(self._expand_conv(inputs)))\n        x = self._swish(self._bn1(self._depthwise_conv(x)))\n\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == [1] and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs\n        return x\n\n    def set_swish(self, memory_efficient=True):\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.332161Z","iopub.execute_input":"2025-06-13T01:12:36.332636Z","iopub.status.idle":"2025-06-13T01:12:36.352031Z","shell.execute_reply.started":"2025-06-13T01:12:36.332618Z","shell.execute_reply":"2025-06-13T01:12:36.351427Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Kelas Utama EfficientNet","metadata":{}},{"cell_type":"code","source":"class EfficientNet(nn.Module):\n    \"\"\" An EfficientNet model. \"\"\"\n    def __init__(self, blocks_args=None, global_params=None, escape=''):\n        super().__init__()\n        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n        assert len(blocks_args) > 0, 'block args must be greater than 0'\n        self.escape = escape\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n        \n        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        in_channels = 3\n        out_channels = round_filters(32, self._global_params)\n        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        self._blocks = nn.ModuleList([])\n        self.stage_map = []\n        stage_count = 0\n        last_block_args = None\n        for block_args in self._blocks_args:\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n            stage_count += 1\n            self.stage_map += [''] * (block_args.num_repeat - 1)\n            self.stage_map.append(f'b{stage_count}')\n            \n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            \n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=[1])\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n            last_block_args = block_args\n\n        in_channels_head = last_block_args.output_filters\n        out_channels_head = round_filters(1280, self._global_params)\n        self._conv_head = Conv2d(in_channels_head, out_channels_head, kernel_size=1, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=out_channels_head, momentum=bn_mom, eps=bn_eps)\n\n        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n        self._fc = nn.Linear(out_channels_head, self._global_params.num_classes)\n        self._swish = MemoryEfficientSwish()\n\n    def set_swish(self, memory_efficient=True):\n        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n        for block in self._blocks:\n            block.set_swish(memory_efficient)\n\n    def extract_features(self, inputs, layers):\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n        layers['b0'] = x\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            stage = self.stage_map[idx]\n            if stage:     \n                layers[stage] = x\n                if stage == self.escape:\n                    return None\n        x = self._swish(self._bn1(self._conv_head(x)))\n        return x\n\n    def forward(self, x):\n        bs = x.size(0)\n        layers = {}\n        x = self.extract_features(x, layers)\n        if x is None:\n            return layers \n        \n        layers['final_conv'] = x\n        \n        x_pool = self._avg_pooling(x)\n        x_pool = x_pool.view(bs, -1)\n        x_pool = self._dropout(x_pool)\n        x_logits = self._fc(x_pool)\n        layers['logits'] = x_logits\n        return layers\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None, escape=''):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return cls(blocks_args, global_params, escape)\n\n    @classmethod\n    def from_pretrained(cls, model_name, advprop=False, num_classes=1000, in_channels=3, escape=''):\n        model = cls.from_name(model_name, override_params={'num_classes': num_classes}, escape=escape)\n        try:\n            load_pretrained_weights(model, model_name, load_fc=(num_classes == 1000), advprop=advprop)\n        except Exception as e:\n            print(f\"Could not load pretrained weights for {model_name}: {e}. Model will be initialized randomly.\")\n\n        if in_channels != 3:\n            Conv2d = get_same_padding_conv2d(image_size = model._global_params.image_size)\n            out_channels = round_filters(32, model._global_params)\n            model._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        return model\n    \n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name):\n        valid_models = [f'efficientnet-b{i}' for i in range(9)] + ['efficientnet-l2']\n        if model_name not in valid_models:\n            raise ValueError(f'model_name should be one of: {\", \".join(valid_models)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.352748Z","iopub.execute_input":"2025-06-13T01:12:36.353001Z","iopub.status.idle":"2025-06-13T01:12:36.370836Z","shell.execute_reply.started":"2025-06-13T01:12:36.352978Z","shell.execute_reply":"2025-06-13T01:12:36.370245Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Arsitektur Xception","metadata":{}},{"cell_type":"code","source":"class SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, in_filters, out_filters, reps, strides=1, start_with_relu=True, grow_first=True):\n        super(Block, self).__init__()\n        if out_filters != in_filters or strides != 1:\n            self.skip = nn.Conv2d(in_filters, out_filters, 1, stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip = None\n\n        rep = []\n        filters = in_filters\n        if grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters, out_filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps - 1):\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(filters, filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(nn.ReLU(inplace=True))\n            rep.append(SeparableConv2d(in_filters, out_filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            if rep:\n                rep = rep[1:]\n        else:\n            if rep:\n                rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3, strides, 1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n        x = x + skip\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.371588Z","iopub.execute_input":"2025-06-13T01:12:36.371878Z","iopub.status.idle":"2025-06-13T01:12:36.388585Z","shell.execute_reply.started":"2025-06-13T01:12:36.371848Z","shell.execute_reply":"2025-06-13T01:12:36.388072Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Kelas Utama Xception","metadata":{}},{"cell_type":"code","source":"class xception_model(nn.Module):\n    \"\"\" Xception optimized for ImageNet. \"\"\"\n    def __init__(self, num_classes=1000, pretrained='imagenet', escape=''):\n        super(xception_model, self).__init__()\n        self.escape = escape\n        self.num_classes = num_classes\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.block1 = Block(64, 128, reps=2, strides=2, start_with_relu=False, grow_first=True)\n        self.block2 = Block(128, 256, reps=2, strides=2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, reps=2, strides=2, start_with_relu=True, grow_first=True)\n        # Middle flow\n        self.block4 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block5 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block6 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block7 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block8 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block9 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, reps=3, strides=1, start_with_relu=True, grow_first=True)\n        # Exit flow\n        self.block12 = Block(728, 1024, reps=2, strides=2, start_with_relu=True, grow_first=False)\n        self.conv3 = SeparableConv2d(1024, 1536, 3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(1536)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.conv4 = SeparableConv2d(1536, 2048, 3, stride=1, padding=1)\n        self.bn4 = nn.BatchNorm2d(2048)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.last_linear = nn.Linear(2048, self.num_classes)\n        \n        # Urutan stage untuk forward pass dan feature extraction\n        self.seq = [\n            ('b0_conv1', [self.conv1, self.bn1, self.relu1]),\n            ('b0_conv2', [self.conv2, self.bn2]),\n            ('b1', [self.relu2, self.block1]),\n            ('b2', [self.block2]), ('b3', [self.block3]), ('b4', [self.block4]),\n            ('b5', [self.block5]), ('b6', [self.block6]), ('b7', [self.block7]),\n            ('b8', [self.block8]), ('b9', [self.block9]), ('b10', [self.block10]),\n            ('b11', [self.block11]), ('b12', [self.block12]),\n            ('final_conv3', [self.conv3, self.bn3, self.relu3]),\n            ('final_conv4', [self.conv4, self.bn4]),\n            ('logits', [self.relu4, self.avgpool, lambda x: x.view(x.size(0), -1), self.last_linear])\n        ]\n        \n        if pretrained == 'imagenet':\n            self.load_imagenet_weights()\n        elif pretrained and isinstance(pretrained, str):\n            self.load_custom_weights(pretrained)\n        else:\n            print(\"Model weights are randomly initialized.\")\n            self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def load_imagenet_weights(self):\n        try:\n            state_dict = model_zoo.load_url('http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth')\n            if self.num_classes != 1000:\n                for key in ['last_linear.weight', 'last_linear.bias']:\n                    if key in state_dict: del state_dict[key]\n            self.load_state_dict(state_dict, strict=False)\n            print(f\"Loaded pretrained ImageNet weights for Xception (num_classes={self.num_classes}).\")\n            if self.num_classes != 1000: print(\"Classifier head (last_linear) re-initialized.\")\n        except Exception as e:\n            print(f\"Could not load pretrained ImageNet weights: {e}. Model weights are randomly initialized.\")\n            self._init_weights()\n\n    def load_custom_weights(self, pretrained_path):\n        try:\n            ckpt = torch.load(pretrained_path, map_location='cpu')\n            state_dict_to_load = ckpt.get('state_dict', ckpt)\n            load_state(self, state_dict_to_load) # Menggunakan helper function\n            print(f\"Loaded pretrained weights from: {pretrained_path}\")\n        except Exception as e:\n            print(f\"Could not load weights from {pretrained_path}: {e}. Model weights are randomly initialized.\")\n            self._init_weights()\n            \n    def forward(self, x):\n        layers = {}\n        current_input = x\n        for stage_name, stage_ops in self.seq:\n            for op in stage_ops:\n                current_input = op(current_input)\n            layers[stage_name] = current_input\n            if stage_name == self.escape:\n                break\n        return layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.389298Z","iopub.execute_input":"2025-06-13T01:12:36.389565Z","iopub.status.idle":"2025-06-13T01:12:36.407912Z","shell.execute_reply.started":"2025-06-13T01:12:36.389549Z","shell.execute_reply":"2025-06-13T01:12:36.407178Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Architektur ConvNeXt","metadata":{}},{"cell_type":"code","source":"try:\n    import timm\nexcept ImportError:\n    logging.error(\"`timm` tidak terinstal. Jalankan `pip install timm`.\")\n    timm = None\n\nclass ConvNeXtBackbone(nn.Module):\n    \"\"\"\n    Wrapper untuk model ConvNeXt dari `timm` agar kompatibel dengan MAT.\n    Model ini akan mengembalikan dictionary fitur dari layer-layer perantara.\n    \"\"\"\n    def __init__(self, model_name='convnext_base.fb_in22k_ft_in1k', pretrained=True, num_classes=2):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Library `timm` diperlukan untuk ConvNeXtBackbone.\")\n            \n        # Muat model ConvNeXt dengan `features_only=True`\n        # Ini akan mengembalikan output dari setiap stage, bukan hanya logits\n        self.net = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            features_only=True,\n            # `out_indices` menentukan output stage mana yang akan dikembalikan\n            # (0,1,2,3) -> 4 stages, resolusi 1/4, 1/8, 1/16, 1/32\n            out_indices=(0, 1, 2, 3) \n        )\n        \n        # Layer head untuk cabang global. Kita tambahkan secara manual.\n        # Ambil jumlah fitur dari stage terakhir\n        self.num_global_features = self.net.feature_info.channels()[-1]\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(self.num_global_features, num_classes)\n        \n        # Mapping nama generik ke nama stage timm\n        # MAT akan menggunakan 's1', 's2', 's3', 's4'\n        self.stage_map = {\n            's1': 0, 's2': 1, 's3': 2, 's4': 3\n        }\n\n    def forward(self, x):\n        # Dapatkan list fitur dari backbone\n        # Outputnya adalah list of tensors, misal: [ (B, C1, H/4, W/4), (B, C2, H/8, W/8), ... ]\n        features_list = self.net(x)\n        \n        # Buat dictionary `layers` yang diharapkan oleh MAT\n        layers = {}\n        for stage_name, idx in self.stage_map.items():\n            layers[stage_name] = features_list[idx]\n\n        # Tambahkan output untuk cabang global (mirip final_conv di EfficientNet)\n        # Kita gunakan fitur dari stage terakhir\n        global_features = features_list[-1]\n        layers['final_conv'] = global_features\n        \n        # Hitung juga logits untuk kompatibilitas jika diperlukan\n        pooled_features = self.global_pool(global_features).flatten(1)\n        logits = self.fc(pooled_features)\n        layers['logits'] = logits\n        \n        return layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:36.408650Z","iopub.execute_input":"2025-06-13T01:12:36.408900Z","iopub.status.idle":"2025-06-13T01:12:40.196950Z","shell.execute_reply.started":"2025-06-13T01:12:36.408878Z","shell.execute_reply":"2025-06-13T01:12:40.196210Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Arsitektur ResNet-101","metadata":{}},{"cell_type":"code","source":"class ResNetBackbone(nn.Module):\n    \"\"\"\n    Wrapper untuk model ResNet dari `timm` agar kompatibel dengan MAT.\n    \"\"\"\n    def __init__(self, model_name='resnet101', pretrained=True, num_classes=2):\n        super().__init__()\n        if timm is None:\n            raise RuntimeError(\"Library `timm` diperlukan untuk ResNetBackbone.\")\n            \n        # Muat model ResNet dengan `features_only=True`\n        self.net = timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            features_only=True,\n            out_indices=(0, 1, 2, 3) # Stage 1, 2, 3, 4\n        )\n        \n        # Layer head untuk cabang global (ditambahkan secara manual)\n        self.num_global_features = self.net.feature_info.channels()[-1]\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(self.num_global_features, num_classes)\n        \n        # Mapping nama generik ke nama stage timm\n        self.stage_map = {'s1': 0, 's2': 1, 's3': 2, 's4': 3}\n\n    def forward(self, x):\n        features_list = self.net(x)\n        \n        # Buat dictionary `layers` yang diharapkan oleh MAT\n        layers = {stage_name: features_list[idx] for stage_name, idx in self.stage_map.items()}\n        \n        # Alias untuk cabang global dan logits\n        global_features = features_list[-1]\n        layers['final_conv'] = global_features\n        layers['logits'] = self.fc(self.global_pool(global_features).flatten(1))\n        \n        return layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.199202Z","iopub.execute_input":"2025-06-13T01:12:40.199535Z","iopub.status.idle":"2025-06-13T01:12:40.205501Z","shell.execute_reply.started":"2025-06-13T01:12:40.199518Z","shell.execute_reply":"2025-06-13T01:12:40.205000Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Arsitektur MAT","metadata":{}},{"cell_type":"markdown","source":"## Attention Map","metadata":{}},{"cell_type":"code","source":"class AttentionMap(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(AttentionMap, self).__init__()\n        self.register_buffer('mask', torch.zeros([1, 1, 24, 24]))\n        if out_channels > 0:\n            self.mask[0, 0, 2:-2, 2:-2] = 1\n        self.num_attentions = out_channels\n        self.conv_extract = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels) if out_channels > 0 else nn.Identity()\n\n    def forward(self, x):\n        if self.num_attentions == 0:\n            return torch.ones([x.shape[0], 1, 1, 1], device=x.device)\n        x = F.relu(self.bn1(self.conv_extract(x)), inplace=True)\n        x = self.bn2(self.conv2(x))\n        x = F.elu(x) + 1\n        mask = self.mask.to(x.device)\n        if x.shape[2:4] != mask.shape[2:4] and mask.sum() > 0:\n            mask = F.interpolate(mask, (x.shape[2], x.shape[3]), mode='nearest')\n        return x * mask\n\nclass AttentionPooling(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, features, attentions, norm=2):\n        H, W = features.shape[-2:]\n        attentions_resized = F.interpolate(attentions, size=(H, W), mode='bilinear', align_corners=True) if (attentions.shape[-2:] != (H,W)) else attentions\n        \n        if len(features.shape) == 4: # (B, C, H, W)\n            feature_matrix = torch.einsum('bmhw,bchw->bmc', attentions_resized, features)\n        elif len(features.shape) == 5: # (B, M, C_per_M, H, W)\n            feature_matrix = torch.einsum('bmhw,bmchw->bmc', attentions_resized, features)\n        else:\n            raise ValueError(f\"Unsupported feature shape: {features.shape}\")\n\n        if norm == 1:\n            att_sum = torch.sum(attentions_resized, dim=(2,3), keepdim=True) + 1e-8\n            normalized_attentions = attentions_resized / att_sum\n            if len(features.shape) == 4:\n                 feature_matrix = torch.einsum('bmhw,bchw->bmc', normalized_attentions, features)\n            else:\n                 feature_matrix = torch.einsum('bmhw,bmchw->bmc', normalized_attentions, features)\n        elif norm == 2:\n            feature_matrix = F.normalize(feature_matrix, p=2, dim=-1)\n        elif norm == 3:\n            w = torch.sum(attentions_resized, dim=(2, 3)).unsqueeze(-1) + 1e-8\n            feature_matrix = feature_matrix / w\n        return feature_matrix\n\nclass Texture_Enhance_v1(nn.Module):\n    def __init__(self,num_features,num_attentions):\n        super().__init__()\n        self.output_features=num_features*4\n        self.output_features_d=num_features\n        self.conv0=nn.Conv2d(num_features,num_features,1)\n        self.bn0 = nn.BatchNorm2d(num_features)\n        self.conv1=nn.Conv2d(num_features,num_features,3,padding=1)\n        self.bn1=nn.BatchNorm2d(num_features)\n        self.conv2=nn.Conv2d(num_features*2,num_features,3,padding=1)\n        self.bn2=nn.BatchNorm2d(num_features)\n        self.conv3=nn.Conv2d(num_features*3,num_features,3,padding=1)\n        self.bn3=nn.BatchNorm2d(num_features)\n        self.conv_last=nn.Conv2d(num_features*4,num_features*4,1)\n        self.bn_last=nn.BatchNorm2d(num_features*4)\n\n    def forward(self,feature_maps,attention_maps=(1,1)):\n        B,N,H,W=feature_maps.shape\n        att_size=(int(H*attention_maps[0]),int(W*attention_maps[1])) if isinstance(attention_maps,tuple) else (attention_maps.shape[2],attention_maps.shape[3])\n        feature_maps_d=F.adaptive_avg_pool2d(feature_maps,att_size)\n        feature_maps_hp = feature_maps - F.interpolate(feature_maps_d,(H,W),mode='bilinear', align_corners=True) if (H>att_size[0] or W>att_size[1]) else feature_maps\n\n        fm0 = F.relu(self.bn0(self.conv0(feature_maps_hp)), inplace=True)\n        fm1 = F.relu(self.bn1(self.conv1(fm0)), inplace=True)\n        fm1_ = torch.cat([fm0,fm1],dim=1)\n        fm2 = F.relu(self.bn2(self.conv2(fm1_)), inplace=True)\n        fm2_ = torch.cat([fm1_,fm2],dim=1)\n        fm3 = F.relu(self.bn3(self.conv3(fm2_)), inplace=True)\n        fm3_ = torch.cat([fm2_,fm3],dim=1)\n        out = F.relu(self.bn_last(self.conv_last(fm3_)), inplace=True)\n        return out, feature_maps_d\n\nclass Texture_Enhance_v2(nn.Module):\n    def __init__(self, num_features, num_attentions):\n        super().__init__()\n        self.N_feat_per_map = num_features\n        self.M = num_attentions\n        self.output_features = self.N_feat_per_map\n        self.output_features_d = self.N_feat_per_map\n        MN = self.M * self.N_feat_per_map\n\n        self.conv_extract = nn.Conv2d(num_features, self.N_feat_per_map, 3, padding=1)\n        self.bn_extract = nn.BatchNorm2d(self.N_feat_per_map)\n        self.conv0 = nn.Conv2d(MN, MN, 5, padding=2, groups=self.M)\n        self.bn0 = nn.BatchNorm2d(MN)\n        self.conv1 = nn.Conv2d(MN, MN, 3, padding=1, groups=self.M)\n        self.bn1 = nn.BatchNorm2d(MN)\n        self.conv2 = nn.Conv2d(MN*2, MN, 3, padding=1, groups=self.M)\n        self.bn2 = nn.BatchNorm2d(MN)\n        self.conv3 = nn.Conv2d(MN*3, MN, 3, padding=1, groups=self.M)\n        self.bn3 = nn.BatchNorm2d(MN)\n        self.bn4 = nn.BatchNorm2d(MN*4)\n        self.conv_last = nn.Conv2d(MN*4, MN, 1, groups=self.M)\n        self.bn_last = nn.BatchNorm2d(MN)\n\n    def cat(self, a, b):\n        B, C_a, H, W = a.shape\n        a_r = a.view(B, self.M, -1, H, W)\n        b_r = b.view(B, self.M, -1, H, W)\n        return torch.cat([a_r, b_r], dim=2).view(B, -1, H, W)\n\n    def forward(self, feature_maps_raw, attention_maps=(1,1)):\n        B, N, H, W = feature_maps_raw.shape\n        att_size = (int(H*attention_maps[0]), int(W*attention_maps[1])) if isinstance(attention_maps, tuple) else (attention_maps.shape[2], attention_maps.shape[3])\n\n        base_feats = F.relu(self.bn_extract(self.conv_extract(feature_maps_raw)), inplace=True)\n        feature_maps_d = F.adaptive_avg_pool2d(base_feats, att_size)\n        feature_maps_hp = base_feats - F.interpolate(feature_maps_d, (H,W), mode='bilinear', align_corners=True) if (H>att_size[0] or W>att_size[1]) else base_feats\n\n        if not isinstance(attention_maps, tuple):\n            att_interp = torch.tanh(F.interpolate(attention_maps.detach(),(H,W),mode='bilinear',align_corners=True))\n            current_fm = torch.cat([feature_maps_hp * att_interp[:, i:i+1] for i in range(self.M)], dim=1)\n        else:\n            current_fm = feature_maps_hp.repeat(1, self.M, 1, 1)\n\n        fm0 = F.relu(self.bn0(self.conv0(current_fm)), inplace=True)\n        fm1 = F.relu(self.bn1(self.conv1(fm0)), inplace=True)\n        fm1_ = self.cat(fm0, fm1)\n        fm2 = F.relu(self.bn2(self.conv2(fm1_)), inplace=True)\n        fm2_ = self.cat(fm1_, fm2)\n        fm3 = F.relu(self.bn3(self.conv3(fm2_)), inplace=True)\n        fm3_ = self.cat(fm2_, fm3)\n        \n        final_conv_in = F.relu(self.bn4(fm3_), inplace=True)\n        final_feats_grouped = F.relu(self.bn_last(self.conv_last(final_conv_in)), inplace=True)\n        return final_feats_grouped.view(B, self.M, self.N_feat_per_map, H, W), feature_maps_d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.206303Z","iopub.execute_input":"2025-06-13T01:12:40.206549Z","iopub.status.idle":"2025-06-13T01:12:40.234485Z","shell.execute_reply.started":"2025-06-13T01:12:40.206530Z","shell.execute_reply":"2025-06-13T01:12:40.233812Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Pooling","metadata":{}},{"cell_type":"code","source":"class AttentionMap(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(AttentionMap, self).__init__()\n        self.register_buffer('mask', torch.zeros([1, 1, 24, 24]))\n        if out_channels > 0:\n            self.mask[0, 0, 2:-2, 2:-2] = 1\n        self.num_attentions = out_channels\n        self.conv_extract = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels) if out_channels > 0 else nn.Identity()\n\n    def forward(self, x):\n        if self.num_attentions == 0:\n            return torch.ones([x.shape[0], 1, 1, 1], device=x.device)\n        x = F.relu(self.bn1(self.conv_extract(x)), inplace=True)\n        x = self.bn2(self.conv2(x))\n        x = F.elu(x) + 1\n        mask = self.mask.to(x.device)\n        if x.shape[2:4] != mask.shape[2:4] and mask.sum() > 0:\n            mask = F.interpolate(mask, (x.shape[2], x.shape[3]), mode='nearest')\n        return x * mask\n\nclass AttentionPooling(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, features, attentions, norm=2):\n        H, W = features.shape[-2:]\n        attentions_resized = F.interpolate(attentions, size=(H, W), mode='bilinear', align_corners=True) if (attentions.shape[-2:] != (H,W)) else attentions\n        \n        if len(features.shape) == 4: # (B, C, H, W)\n            feature_matrix = torch.einsum('bmhw,bchw->bmc', attentions_resized, features)\n        elif len(features.shape) == 5: # (B, M, C_per_M, H, W)\n            feature_matrix = torch.einsum('bmhw,bmchw->bmc', attentions_resized, features)\n        else:\n            raise ValueError(f\"Unsupported feature shape: {features.shape}\")\n\n        if norm == 1:\n            att_sum = torch.sum(attentions_resized, dim=(2,3), keepdim=True) + 1e-8\n            normalized_attentions = attentions_resized / att_sum\n            if len(features.shape) == 4:\n                 feature_matrix = torch.einsum('bmhw,bchw->bmc', normalized_attentions, features)\n            else:\n                 feature_matrix = torch.einsum('bmhw,bmchw->bmc', normalized_attentions, features)\n        elif norm == 2:\n            feature_matrix = F.normalize(feature_matrix, p=2, dim=-1)\n        elif norm == 3:\n            w = torch.sum(attentions_resized, dim=(2, 3)).unsqueeze(-1) + 1e-8\n            feature_matrix = feature_matrix / w\n        return feature_matrix\n\nclass Texture_Enhance_v1(nn.Module):\n    def __init__(self,num_features,num_attentions):\n        super().__init__()\n        self.output_features=num_features*4\n        self.output_features_d=num_features\n        self.conv0=nn.Conv2d(num_features,num_features,1)\n        self.bn0 = nn.BatchNorm2d(num_features)\n        self.conv1=nn.Conv2d(num_features,num_features,3,padding=1)\n        self.bn1=nn.BatchNorm2d(num_features)\n        self.conv2=nn.Conv2d(num_features*2,num_features,3,padding=1)\n        self.bn2=nn.BatchNorm2d(num_features)\n        self.conv3=nn.Conv2d(num_features*3,num_features,3,padding=1)\n        self.bn3=nn.BatchNorm2d(num_features)\n        self.conv_last=nn.Conv2d(num_features*4,num_features*4,1)\n        self.bn_last=nn.BatchNorm2d(num_features*4)\n\n    def forward(self,feature_maps,attention_maps=(1,1)):\n        B,N,H,W=feature_maps.shape\n        att_size=(int(H*attention_maps[0]),int(W*attention_maps[1])) if isinstance(attention_maps,tuple) else (attention_maps.shape[2],attention_maps.shape[3])\n        feature_maps_d=F.adaptive_avg_pool2d(feature_maps,att_size)\n        feature_maps_hp = feature_maps - F.interpolate(feature_maps_d,(H,W),mode='bilinear', align_corners=True) if (H>att_size[0] or W>att_size[1]) else feature_maps\n\n        fm0 = F.relu(self.bn0(self.conv0(feature_maps_hp)), inplace=True)\n        fm1 = F.relu(self.bn1(self.conv1(fm0)), inplace=True)\n        fm1_ = torch.cat([fm0,fm1],dim=1)\n        fm2 = F.relu(self.bn2(self.conv2(fm1_)), inplace=True)\n        fm2_ = torch.cat([fm1_,fm2],dim=1)\n        fm3 = F.relu(self.bn3(self.conv3(fm2_)), inplace=True)\n        fm3_ = torch.cat([fm2_,fm3],dim=1)\n        out = F.relu(self.bn_last(self.conv_last(fm3_)), inplace=True)\n        return out, feature_maps_d\n\nclass Texture_Enhance_v2(nn.Module):\n    def __init__(self, num_features, num_attentions):\n        super().__init__()\n        self.N_feat_per_map = num_features\n        self.M = num_attentions\n        self.output_features = self.N_feat_per_map\n        self.output_features_d = self.N_feat_per_map\n        MN = self.M * self.N_feat_per_map\n\n        self.conv_extract = nn.Conv2d(num_features, self.N_feat_per_map, 3, padding=1)\n        self.bn_extract = nn.BatchNorm2d(self.N_feat_per_map)\n        self.conv0 = nn.Conv2d(MN, MN, 5, padding=2, groups=self.M)\n        self.bn0 = nn.BatchNorm2d(MN)\n        self.conv1 = nn.Conv2d(MN, MN, 3, padding=1, groups=self.M)\n        self.bn1 = nn.BatchNorm2d(MN)\n        self.conv2 = nn.Conv2d(MN*2, MN, 3, padding=1, groups=self.M)\n        self.bn2 = nn.BatchNorm2d(MN)\n        self.conv3 = nn.Conv2d(MN*3, MN, 3, padding=1, groups=self.M)\n        self.bn3 = nn.BatchNorm2d(MN)\n        self.bn4 = nn.BatchNorm2d(MN*4)\n        self.conv_last = nn.Conv2d(MN*4, MN, 1, groups=self.M)\n        self.bn_last = nn.BatchNorm2d(MN)\n\n    def cat(self, a, b):\n        B, C_a, H, W = a.shape\n        a_r = a.view(B, self.M, -1, H, W)\n        b_r = b.view(B, self.M, -1, H, W)\n        return torch.cat([a_r, b_r], dim=2).view(B, -1, H, W)\n\n    def forward(self, feature_maps_raw, attention_maps=(1,1)):\n        B, N, H, W = feature_maps_raw.shape\n        att_size = (int(H*attention_maps[0]), int(W*attention_maps[1])) if isinstance(attention_maps, tuple) else (attention_maps.shape[2], attention_maps.shape[3])\n\n        base_feats = F.relu(self.bn_extract(self.conv_extract(feature_maps_raw)), inplace=True)\n        feature_maps_d = F.adaptive_avg_pool2d(base_feats, att_size)\n        feature_maps_hp = base_feats - F.interpolate(feature_maps_d, (H,W), mode='bilinear', align_corners=True) if (H>att_size[0] or W>att_size[1]) else base_feats\n\n        if not isinstance(attention_maps, tuple):\n            att_interp = torch.tanh(F.interpolate(attention_maps.detach(),(H,W),mode='bilinear',align_corners=True))\n            current_fm = torch.cat([feature_maps_hp * att_interp[:, i:i+1] for i in range(self.M)], dim=1)\n        else:\n            current_fm = feature_maps_hp.repeat(1, self.M, 1, 1)\n\n        fm0 = F.relu(self.bn0(self.conv0(current_fm)), inplace=True)\n        fm1 = F.relu(self.bn1(self.conv1(fm0)), inplace=True)\n        fm1_ = self.cat(fm0, fm1)\n        fm2 = F.relu(self.bn2(self.conv2(fm1_)), inplace=True)\n        fm2_ = self.cat(fm1_, fm2)\n        fm3 = F.relu(self.bn3(self.conv3(fm2_)), inplace=True)\n        fm3_ = self.cat(fm2_, fm3)\n        \n        final_conv_in = F.relu(self.bn4(fm3_), inplace=True)\n        final_feats_grouped = F.relu(self.bn_last(self.conv_last(final_conv_in)), inplace=True)\n        return final_feats_grouped.view(B, self.M, self.N_feat_per_map, H, W), feature_maps_d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.235174Z","iopub.execute_input":"2025-06-13T01:12:40.235405Z","iopub.status.idle":"2025-06-13T01:12:40.258533Z","shell.execute_reply.started":"2025-06-13T01:12:40.235380Z","shell.execute_reply":"2025-06-13T01:12:40.257815Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Texture Enhancement","metadata":{}},{"cell_type":"code","source":"class Auxiliary_Loss_v1(nn.Module):\n    def __init__(self, M, N_feat_per_map, C, alpha=0.05, margin=1, inner_margin=[0.01, 0.02]):\n        super().__init__()\n        self.M, self.N_feat_per_map, self.num_classes = M, N_feat_per_map, C\n        self.register_buffer('feature_centers', torch.zeros(M, N_feat_per_map))\n        self.register_buffer('alpha', torch.tensor(alpha))\n        self.margin = margin\n        self.atp = AttentionPooling()\n        \n        im = torch.tensor(inner_margin)\n        if len(im) != C: im = im[0].repeat(C) if len(im)>0 else torch.tensor([0.01]*C)\n        self.register_buffer('inner_margin', im)\n\n    def forward(self, feature_map_d, attentions, y):\n        feature_matrix = self.atp(feature_map_d, attentions)\n        fcts_detached = self.feature_centers.detach()\n        center_momentum = feature_matrix - fcts_detached.unsqueeze(0)\n        fcts_update = self.alpha * torch.mean(center_momentum, dim=0) + fcts_detached\n        if self.training:\n            with torch.no_grad(): self.feature_centers = fcts_update.detach()\n\n        current_inner_margin = self.inner_margin[y]\n        dist_to_centers = torch.norm(feature_matrix - fcts_update.unsqueeze(0), dim=-1)\n        intra_loss = torch.mean(F.relu(dist_to_centers - current_inner_margin.unsqueeze(1)))\n\n        inter_loss = 0\n        if self.M > 1:\n            for j in range(self.M):\n                for k in range(j + 1, self.M):\n                    inter_loss += F.relu(self.margin - torch.dist(fcts_update[j, :], fcts_update[k, :]))\n            inter_loss /= (self.M * (self.M - 1) / 2 + 1e-8)\n        return intra_loss + inter_loss, feature_matrix\n\n\nclass Auxiliary_Loss_v2(nn.Module):\n    def __init__(self, M, N_feat_per_map, C, alpha=0.05, margin=1, inner_margin=[0.1, 0.5]):\n        super().__init__()\n        self.M, self.N_feat_per_map, self.num_classes = M, N_feat_per_map, C\n        self.register_buffer('feature_centers', torch.zeros(M, N_feat_per_map))\n        self.register_buffer('alpha', torch.tensor(alpha))\n        self.margin = margin\n        self.atp = AttentionPooling()\n\n        im = torch.tensor(inner_margin)\n        if len(im) != C: im = im[0].repeat(C) if len(im)>0 else torch.tensor([0.1]*C)\n        self.register_buffer('inner_margin', im)\n\n    def forward(self, feature_map_d, attentions, y):\n        feature_matrix = self.atp(feature_map_d, attentions)\n        fcts_detached = self.feature_centers.detach()\n        center_momentum = feature_matrix - fcts_detached.unsqueeze(0)\n\n        real_mask = (y == 0).view(-1, 1, 1).float()\n        num_real = real_mask.sum() + 1e-8\n        mean_momentum = torch.sum(center_momentum * real_mask, dim=0) / num_real if num_real > 1e-7 else torch.zeros_like(fcts_detached)\n        fcts_update = self.alpha * mean_momentum + fcts_detached\n        if self.training:\n            with torch.no_grad(): self.feature_centers = fcts_update.detach()\n        \n        target_dist = self.inner_margin[y]\n        dist_overall = torch.norm(feature_matrix - fcts_update.unsqueeze(0), dim=[1, 2])\n        loss_real = F.relu(dist_overall - target_dist) * (y == 0).float()\n        loss_fake = F.relu(target_dist - dist_overall) * (y == 1).float()\n        intra_loss = torch.mean(loss_real + loss_fake)\n\n        inter_loss = 0\n        if self.M > 1:\n            for j in range(self.M):\n                for k in range(j + 1, self.M):\n                    inter_loss += F.relu(self.margin - torch.dist(fcts_update[j, :], fcts_update[k, :]))\n            inter_loss /= (self.M * (self.M - 1) / 2 + 1e-8)\n        return intra_loss + inter_loss, feature_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.259240Z","iopub.execute_input":"2025-06-13T01:12:40.259450Z","iopub.status.idle":"2025-06-13T01:12:40.277507Z","shell.execute_reply.started":"2025-06-13T01:12:40.259418Z","shell.execute_reply":"2025-06-13T01:12:40.277030Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Kelas Utama MAT","metadata":{}},{"cell_type":"code","source":"class MAT(nn.Module):\n    def __init__(self, net='xception_model',feature_layer='b3',attention_layer='final_conv',num_classes=2, M=8,mid_dims=256,\n                 dropout_rate=0.5,drop_final_rate=0.5, pretrained_backbone=False, pretrained_mat=None,\n                 alpha=0.05,size=(380,380),margin=1,inner_margin=[0.01,0.02],\n                 aux_loss_ver=1, texture_enhance_ver=2):\n        super(MAT, self).__init__()\n        self.num_classes, self.M = num_classes, M\n        \n        logging.info(f\"MAT: Initializing backbone '{net}'...\")\n\n        if 'resnet' in net or 'resnext' in net:\n            self.net = ResNetBackbone(model_name=net, pretrained=pretrained_backbone, num_classes=num_classes)\n            self.feature_layer = 's2'\n            self.attention_layer = 's3'\n            self.GLOBAL_BRANCH_FEATURE_LAYER_NAME = 'final_conv'\n        elif 'convnext' in net:\n            self.net = ConvNeXtBackbone(model_name=net, pretrained=pretrained_backbone, num_classes=num_classes)\n            self.feature_layer = 's2'  # Fitur dari stage 2 (resolusi 1/8)\n            self.attention_layer = 's3' # Atensi dari stage 3 (resolusi 1/16)\n            self.GLOBAL_BRANCH_FEATURE_LAYER_NAME = 'final_conv' # Di ConvNeXtWrapper, ini adalah stage 4\n        elif 'xception' in net:\n            self.net = xception_model(num_classes=num_classes, pretrained='imagenet' if pretrained_backbone else None)\n            self.feature_layer = feature_layer\n            self.attention_layer = attention_layer\n            self.GLOBAL_BRANCH_FEATURE_LAYER_NAME = 'final_conv4'\n        elif 'efficientnet' in net:\n            self.net = EfficientNet.from_pretrained(net, advprop=True, num_classes=num_classes) if pretrained_backbone else EfficientNet.from_name(net, override_params={'num_classes': num_classes})\n            self.feature_layer = feature_layer\n            self.attention_layer = attention_layer\n            self.GLOBAL_BRANCH_FEATURE_LAYER_NAME = 'final_conv'\n        else:\n            raise ValueError(f\"Unsupported backbone: {net}\")\n\n        logging.info(\"MAT: Dry running backbone to get feature shapes...\")\n        # Ukuran input untuk dry run\n        dry_run_size = size\n        with torch.no_grad():\n            # Semua backbone wrapper kita sekarang mengembalikan dict\n            layers = self.net(torch.zeros(1, 3, *dry_run_size))\n        \n        # Ambil jumlah channel dari output dry run\n        num_feat_backbone = layers[self.feature_layer].shape[1]\n        att_in_channels = layers[self.attention_layer].shape[1]\n        global_in_channels = layers[self.GLOBAL_BRANCH_FEATURE_LAYER_NAME].shape[1]\n        \n        logging.info(f\"  > Feature layer '{self.feature_layer}' channels: {num_feat_backbone}\")\n        logging.info(f\"  > Attention layer '{self.attention_layer}' channels: {att_in_channels}\")\n        logging.info(f\"  > Global branch layer '{self.GLOBAL_BRANCH_FEATURE_LAYER_NAME}' channels: {global_in_channels}\")\n\n        # --- Sisa arsitektur MAT (tidak perlu diubah) ---\n        self.attentions = AttentionMap(att_in_channels, self.M)\n        self.atp = AttentionPooling()\n        \n        logging.info(f\"MAT: Using TextureEnhance v{texture_enhance_ver}\")\n        if texture_enhance_ver == 1: self.texture_enhance = Texture_Enhance_v1(num_feat_backbone, self.M)\n        elif texture_enhance_ver == 2: self.texture_enhance = Texture_Enhance_v2(num_feat_backbone, self.M)\n        else: raise ValueError(f\"Invalid TextureEnhance version: {texture_enhance_ver}\")\n        \n        feat_per_map_te = self.texture_enhance.output_features // (self.M if texture_enhance_ver == 1 else 1)\n        \n        self.projection_local = nn.Sequential(nn.Linear(self.M * feat_per_map_te, mid_dims), nn.Hardswish(), nn.Linear(mid_dims,mid_dims))\n        self.project_final = nn.Linear(global_in_channels, mid_dims)\n        self.ensemble_classifier_fc = nn.Sequential(nn.Linear(mid_dims*2, mid_dims), nn.Hardswish(), nn.Linear(mid_dims, num_classes))\n\n        logging.info(f\"MAT: Using AuxiliaryLoss v{aux_loss_ver}\")\n        AuxLoss = Auxiliary_Loss_v1 if aux_loss_ver==1 else Auxiliary_Loss_v2\n        self.auxiliary_loss = AuxLoss(M, self.texture_enhance.output_features_d, num_classes, alpha, margin, inner_margin)\n\n        self.dropout = nn.Dropout2d(dropout_rate, inplace=True)\n        self.dropout_final = nn.Dropout(drop_final_rate, inplace=True)\n\n        if pretrained_mat:\n            logging.info(f\"MAT: Loading MAT weights from {pretrained_mat}\")\n            try:\n                mat_ckpt = torch.load(pretrained_mat, map_location='cpu')\n                load_state(self, mat_ckpt.get('state_dict', mat_ckpt))\n            except Exception as e:\n                logging.error(f\"MAT: Error loading MAT weights: {e}\")\n\n    def train_batch(self, x, y, jump_aux=False, drop_final_rate_override=None):\n        self.train()\n        layers = self.net(x)\n        \n        feature_maps = layers[self.feature_layer]\n        raw_att_feats = layers[self.attention_layer]\n        attention_maps = self.attentions(raw_att_feats)\n\n        dropout_mask = self.dropout(torch.ones(attention_maps.shape[0], self.M, 1, 1, device=x.device))\n        att_maps_train = attention_maps * dropout_mask\n        enhanced_feats, low_freq_d = self.texture_enhance(feature_maps, att_maps_train)\n        low_freq_d_norm = (low_freq_d - low_freq_d.mean(dim=[2,3],keepdim=True)) / (torch.std(low_freq_d,dim=[2,3],keepdim=True)+1e-8)\n        feat_matrix_pooled = self.atp(enhanced_feats, attention_maps)\n        feat_matrix_local_drop = (feat_matrix_pooled * dropout_mask.squeeze(-1)).view(x.size(0), -1)\n        projected_local = F.hardswish(self.projection_local(feat_matrix_local_drop))\n        aux_loss, feat_matrix_d_loss = (torch.tensor(0.0, device=x.device), self.atp(low_freq_d_norm, attention_maps)) if jump_aux else self.auxiliary_loss(low_freq_d_norm, attention_maps, y)\n        global_feats = layers[self.GLOBAL_BRANCH_FEATURE_LAYER_NAME]\n        summed_att = att_maps_train.sum(dim=1, keepdim=True)\n        pooled_global = self.atp(global_feats, summed_att, norm=1).squeeze(1)\n        drop_rate = drop_final_rate_override if drop_final_rate_override is not None else self.dropout_final.p\n        dropped_global = nn.functional.dropout(pooled_global, p=drop_rate, training=self.training, inplace=True)\n        projected_global = F.hardswish(self.project_final(dropped_global))\n        combined = torch.cat((projected_local, projected_global), dim=1)\n        logit = self.ensemble_classifier_fc(combined)\n        loss = F.cross_entropy(logit, y)\n        return dict(ensemble_loss=loss, aux_loss=aux_loss, attention_maps=attention_maps, ensemble_logit=logit,\n                    feature_matrix_local_raw=feat_matrix_pooled, feature_matrix_lowfreq_d=feat_matrix_d_loss)\n\n\n    def forward(self, x, y=None, train_batch_flag=False, **kwargs):\n        if self.training or train_batch_flag:\n            return self.train_batch(x, y, **kwargs)\n        \n        self.eval()\n        with torch.no_grad():\n            layers = self.net(x)\n            attention_maps = self.attentions(layers[self.attention_layer])\n            enhanced_feats, _ = self.texture_enhance(layers[self.feature_layer], attention_maps)\n            \n            feat_matrix_pooled = self.atp(enhanced_feats, attention_maps).view(x.size(0), -1)\n            projected_local = F.hardswish(self.projection_local(feat_matrix_pooled))\n            \n            global_feats = layers[self.GLOBAL_BRANCH_FEATURE_LAYER_NAME]\n            summed_att = attention_maps.sum(dim=1, keepdim=True)\n            pooled_global = self.atp(global_feats, summed_att, norm=1).squeeze(1)\n            projected_global = F.hardswish(self.project_final(pooled_global))\n            \n            combined = torch.cat((projected_local, projected_global), dim=1)\n            return self.ensemble_classifier_fc(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.278056Z","iopub.execute_input":"2025-06-13T01:12:40.278211Z","iopub.status.idle":"2025-06-13T01:12:40.297113Z","shell.execute_reply.started":"2025-06-13T01:12:40.278198Z","shell.execute_reply":"2025-06-13T01:12:40.296429Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Arsitektur netrunc","metadata":{}},{"cell_type":"code","source":"class netrunc(nn.Module):\n    def __init__(self, net_arch='xception_model', feature_layer='b3', num_classes=2, dropout_rate=0.5,\n                 pretrained_backbone=False, pretrained_netrunc=None, input_size=(100,100)):\n        super().__init__()\n        self.num_classes, self.feature_layer = num_classes, feature_layer\n        logging.info(f\"netrunc: Initializing backbone '{net_arch}' with escape at '{feature_layer}'\")\n\n        if 'xception' in net_arch:\n            self.net = xception_model(num_classes=num_classes, escape=feature_layer, pretrained='imagenet' if pretrained_backbone else None)\n        elif 'efficientnet' in net_arch:\n            self.net = EfficientNet.from_pretrained(net_arch, advprop=True, num_classes=num_classes, escape=feature_layer) if pretrained_backbone else EfficientNet.from_name(net_arch, override_params={'num_classes':num_classes}, escape=feature_layer)\n        else:\n            raise ValueError(f\"Unsupported backbone for netrunc: {net_arch}\")\n\n        self.net.eval()\n        dry_run_size = (EfficientNet.get_image_size(net_arch), EfficientNet.get_image_size(net_arch)) if 'efficientnet' in net_arch else (299,299)\n        with torch.no_grad():\n            layers_output = self.net(torch.zeros(1, 3, *dry_run_size))\n        \n        num_feat_backbone = layers_output[self.feature_layer].shape[1]\n        \n        self.texture_enhance = Texture_Enhance_v2(num_feat_backbone, num_attentions=1)\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(self.texture_enhance.output_features, self.num_classes)\n        self.dropout = nn.Dropout(dropout_rate)\n\n        if pretrained_netrunc:\n            logging.info(f\"netrunc: Loading weights from {pretrained_netrunc}\")\n            try:\n                ckpt = torch.load(pretrained_netrunc, map_location='cpu')\n                load_state(self, ckpt.get('state_dict', ckpt))\n            except Exception as e:\n                logging.error(f\"netrunc: Error loading weights: {e}\")\n\n    def forward(self, x):\n        self.net.train(self.training)\n        feature_maps = self.net(x)[self.feature_layer]\n        enhanced_fm, _ = self.texture_enhance(feature_maps, attention_maps=(0.2, 0.2))\n        pooled_fm = self.pooling(enhanced_fm.squeeze(1)).flatten(start_dim=1)\n        dropped_fm = self.dropout(pooled_fm)\n        return self.fc(dropped_fm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.297807Z","iopub.execute_input":"2025-06-13T01:12:40.298021Z","iopub.status.idle":"2025-06-13T01:12:40.315351Z","shell.execute_reply.started":"2025-06-13T01:12:40.297998Z","shell.execute_reply":"2025-06-13T01:12:40.314662Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# Modul Augmentasi Data (AGDA)","metadata":{}},{"cell_type":"code","source":"class AGDA(nn.Module):\n    def __init__(self, kernel_size=7, dilation=2, sigma=5, threshold=(0.4, 0.6), zoom=(3, 5), scale_factor=0.5, noise_rate=0.1, mode='soft'):\n        super().__init__()\n        self.kernel_size, self.dilation, self.sigma = kernel_size, dilation, sigma\n        self.noise_rate, self.scale_factor = noise_rate, scale_factor\n        self.threshold, self.zoom, self.mode = threshold, zoom, mode\n        self.filter = kornia.filters.GaussianBlur2d((self.kernel_size, self.kernel_size), (self.sigma, self.sigma))\n\n    def mod_func(self, x):\n        thres = random.uniform(*self.threshold) if isinstance(self.threshold, tuple) else self.threshold\n        zoom = random.uniform(*self.zoom) if isinstance(self.zoom, tuple) else self.zoom\n        bottom = torch.sigmoid((torch.tensor(0.) - thres) * zoom)\n        return (torch.sigmoid((x - thres) * zoom) - bottom) / (1 - bottom)\n\n    def soft_drop(self, x, attention_map):\n        with torch.no_grad():\n            attention_map = self.mod_func(attention_map)\n            B, C, H, W = x.size()\n            xs = F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=True)\n            xs = self.filter(xs)\n            xs += torch.randn_like(xs) * self.noise_rate\n            xs = F.interpolate(xs, (H, W), mode='bilinear', align_corners=True)\n            return x * (1 - attention_map) + xs * attention_map\n\n    def hard_drop(self, X, attention_map):\n        with torch.no_grad():\n            thres = random.uniform(*self.threshold) if isinstance(self.threshold, tuple) else self.threshold\n            attention_mask = (attention_map < thres).float()\n            return attention_mask * X\n            \n    def agda(self, X, attention_map):\n        with torch.no_grad():\n            B, M, AH, AW = attention_map.shape\n            H, W = X.shape[-2:]\n            \n            attention_weight = torch.sum(attention_map, dim=(2, 3))\n            attention_map_interp = F.interpolate(attention_map, (H, W), mode=\"bilinear\", align_corners=True)\n            \n            # Sample satu attention map per gambar di batch\n            index = torch.distributions.categorical.Categorical(torch.sqrt(attention_weight + 1)).sample()\n            index_reshaped = index.view(-1, 1, 1, 1).repeat(1, 1, H, W)\n            \n            # Pilih attention map yang sesuai\n            selected_attention_map = torch.gather(attention_map_interp, 1, index_reshaped)\n            \n            # Normalisasi\n            max_vals = torch.max(selected_attention_map.view(B, -1), dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n            normalized_map = selected_attention_map / (max_vals + 1e-8)\n\n            if self.mode == 'soft':\n                return self.soft_drop(X, normalized_map), index\n            elif self.mode == 'hard':\n                return self.hard_drop(X, normalized_map), index\n            elif self.mode == 'mix':\n                drop_fn = self.soft_drop if random.random() < 0.5 else self.hard_drop\n                return drop_fn(X, normalized_map), index\n            else:\n                raise NotImplementedError(f\"AGDA mode '{self.mode}' not recognized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.316097Z","iopub.execute_input":"2025-06-13T01:12:40.316334Z","iopub.status.idle":"2025-06-13T01:12:40.333405Z","shell.execute_reply.started":"2025-06-13T01:12:40.316313Z","shell.execute_reply":"2025-06-13T01:12:40.332712Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = MAT(\n    net='efficientnet-b4',\n    feature_layer='b3',\n    attention_layer='b5',\n    num_classes=2,\n    pretrained_backbone=True\n)\nprint(\"Model MAT berhasil dibuat.\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:40.334222Z","iopub.execute_input":"2025-06-13T01:12:40.334454Z","iopub.status.idle":"2025-06-13T01:12:48.218818Z","shell.execute_reply.started":"2025-06-13T01:12:40.334428Z","shell.execute_reply":"2025-06-13T01:12:48.218034Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/adv-efficientnet-b4-44fb3a87.pth\" to /root/.cache/torch/hub/checkpoints/adv-efficientnet-b4-44fb3a87.pth\n100%|| 74.4M/74.4M [00:05<00:00, 13.6MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Could not load pretrained weights for efficientnet-b4: Missing keys: ['_fc.weight', '_fc.bias']. Model will be initialized randomly.\nModel MAT berhasil dibuat.\nMAT(\n  (net): EfficientNet(\n    (_conv_stem): Conv2dStaticSamePadding(\n      3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n      (static_padding): ZeroPad2d((0, 1, 0, 1))\n    )\n    (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (_blocks): ModuleList(\n      (0): MBConvBlock(\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          48, 12, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          12, 48, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (1): MBConvBlock(\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          24, 24, kernel_size=(3, 3), stride=[1, 1], groups=24, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          24, 6, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          6, 24, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (2): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n          (static_padding): ZeroPad2d((0, 1, 0, 1))\n        )\n        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          144, 6, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          6, 144, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (3-5): 3 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          192, 192, kernel_size=(3, 3), stride=[1, 1], groups=192, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          192, 8, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          8, 192, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (6): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n          (static_padding): ZeroPad2d((1, 2, 1, 2))\n        )\n        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          192, 8, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          8, 192, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (7-9): 3 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          336, 336, kernel_size=(5, 5), stride=[1, 1], groups=336, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          336, 14, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          14, 336, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (10): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n          (static_padding): ZeroPad2d((0, 1, 0, 1))\n        )\n        (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          336, 14, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          14, 336, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (11-15): 5 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          672, 672, kernel_size=(3, 3), stride=[1, 1], groups=672, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          672, 28, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          28, 672, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (16): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          672, 28, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          28, 672, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (17-21): 5 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          960, 40, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          40, 960, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (22): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n          (static_padding): ZeroPad2d((1, 2, 1, 2))\n        )\n        (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          960, 40, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          40, 960, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (23-29): 7 x MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          1632, 1632, kernel_size=(5, 5), stride=[1, 1], groups=1632, bias=False\n          (static_padding): ZeroPad2d((2, 2, 2, 2))\n        )\n        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (30): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          1632, 68, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          68, 1632, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n      (31): MBConvBlock(\n        (_expand_conv): Conv2dStaticSamePadding(\n          448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_depthwise_conv): Conv2dStaticSamePadding(\n          2688, 2688, kernel_size=(3, 3), stride=[1, 1], groups=2688, bias=False\n          (static_padding): ZeroPad2d((1, 1, 1, 1))\n        )\n        (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_se_reduce): Conv2dStaticSamePadding(\n          2688, 112, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_se_expand): Conv2dStaticSamePadding(\n          112, 2688, kernel_size=(1, 1), stride=(1, 1)\n          (static_padding): Identity()\n        )\n        (_project_conv): Conv2dStaticSamePadding(\n          2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n          (static_padding): Identity()\n        )\n        (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n        (_swish): MemoryEfficientSwish()\n      )\n    )\n    (_conv_head): Conv2dStaticSamePadding(\n      448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n      (static_padding): Identity()\n    )\n    (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n    (_dropout): Dropout(p=0.4, inplace=False)\n    (_fc): Linear(in_features=1792, out_features=2, bias=True)\n    (_swish): MemoryEfficientSwish()\n  )\n  (attentions): AttentionMap(\n    (conv_extract): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(160, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (atp): AttentionPooling()\n  (texture_enhance): Texture_Enhance_v2(\n    (conv_extract): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (bn_extract): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv0): Conv2d(448, 448, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=8)\n    (bn0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv1): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n    (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(896, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n    (bn2): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(1344, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n    (bn3): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (bn4): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv_last): Conv2d(1792, 448, kernel_size=(1, 1), stride=(1, 1), groups=8)\n    (bn_last): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (projection_local): Sequential(\n    (0): Linear(in_features=448, out_features=256, bias=True)\n    (1): Hardswish()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n  )\n  (project_final): Linear(in_features=1792, out_features=256, bias=True)\n  (ensemble_classifier_fc): Sequential(\n    (0): Linear(in_features=512, out_features=256, bias=True)\n    (1): Hardswish()\n    (2): Linear(in_features=256, out_features=2, bias=True)\n  )\n  (auxiliary_loss): Auxiliary_Loss_v1(\n    (atp): AttentionPooling()\n  )\n  (dropout): Dropout2d(p=0.5, inplace=True)\n  (dropout_final): Dropout(p=0.5, inplace=True)\n)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"markdown","source":"## Konfigurasi Eksperimen","metadata":{}},{"cell_type":"code","source":"# Cell 1: Setup, Impor Pustaka, dan Konfigurasi Awal\n\nimport os\nimport time\nimport logging\nimport warnings\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport kornia\n\nimport cv2\n\n# --- Pengaturan Awal ---\n# Mengoptimalkan performa OpenCV dan menghindari konflik thread dengan PyTorch\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\n# Mengabaikan peringatan yang tidak krusial\nwarnings.filterwarnings(\"ignore\")\n\n# Konfigurasi logging agar berfungsi dengan baik di notebook\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n\n# Menentukan device\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogging.info(f\"Menggunakan device: {DEVICE}\")\n\n# Atur seed untuk reproduktifitas\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # Pengaturan ini dapat memperlambat training, tapi hasilnya lebih konsisten\n        # torch.backends.cudnn.deterministic = True\n        # torch.backends.cudnn.benchmark = False\n\nset_seed(12345)\nlogging.info(\"Seed telah diatur untuk reproduktifitas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.219679Z","iopub.execute_input":"2025-06-13T01:12:48.219982Z","iopub.status.idle":"2025-06-13T01:12:48.772923Z","shell.execute_reply.started":"2025-06-13T01:12:48.219964Z","shell.execute_reply":"2025-06-13T01:12:48.772392Z"}},"outputs":[{"name":"stderr","text":"2025-06-13 01:12:48,765 - INFO - Menggunakan device: cuda\n2025-06-13 01:12:48,770 - INFO - Seed telah diatur untuk reproduktifitas.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        # --- Pengaturan Umum ---\n        self.name = \"ffpp_mat_resnet101_v1\"\n        self.epochs = 3\n        self.batch_size = 16 # Sesuaikan dengan VRAM GPU \n        self.workers = 1   # Sesuaikan dengan jumlah core CPU\n        self.learning_rate = 1e-4\n        self.scheduler_step = 1\n        self.scheduler_gamma = 0.95\n        \n        # --- Path Data & Checkpoint ---\n        self.labels_csv_path = '/kaggle/input/ffpp-fix/labels.csv'\n        self.images_folder_path = '/kaggle/input/ffpp-fix/'\n        self.checkpoint_dir = f'/kaggle/working/checkpoints/{self.name}'\n        self.resume_checkpoint = None \n        \n        # Pastikan direktori checkpoint ada\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        \n        # --- Konfigurasi Model (MAT) ---\n        self.net_config = {\n            'net': 'resnet101', # ukuran gambar harus sesuai\n            'feature_layer': 'b3',\n            'attention_layer': 'b5',\n            'num_classes': 2,\n            'M': 4, # Jumlah attention maps\n            'mid_dims': 256,\n            'dropout_rate': 0.3,\n            'drop_final_rate': 0.3,\n            'pretrained_backbone': True,\n            'pretrained_mat': None,\n            'alpha': 0.05,\n            'size': (224, 224), # Sesuai dengan backbone\n            'margin': 1.0,\n            'inner_margin': [0.01, 0.02],\n            'aux_loss_ver': 1,\n            'texture_enhance_ver': 2\n        }\n\n        # --- Konfigurasi Augmentasi (AGDA) ---\n        self.AGDA_config = {\n            'kernel_size': 7, 'dilation': 2, 'sigma': 5,\n            'threshold': (0.3, 0.7), 'zoom': (3, 5),\n            'scale_factor': 0.5, 'noise_rate': 0.1, 'mode': 'soft'\n        }\n        \n        # --- Bobot Loss ---\n        self.ensemble_loss_weight = 1.0\n        self.aux_loss_weight = 0.1\n        self.AGDA_loss_weight = 0.1\n        self.match_loss_weight = 0.01\n\n# Buat instance dari config\nconfig = Config()\n\n# Verifikasi path data\nif not os.path.exists(config.labels_csv_path) or not os.path.exists(config.images_folder_path):\n    logging.error(\"Path dataset tidak valid! Periksa `config.labels_csv_path` dan `config.images_folder_path`.\")\nelse:\n    logging.info(\"Path dataset ditemukan.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.773557Z","iopub.execute_input":"2025-06-13T01:12:48.773912Z","iopub.status.idle":"2025-06-13T01:12:48.794336Z","shell.execute_reply.started":"2025-06-13T01:12:48.773895Z","shell.execute_reply":"2025-06-13T01:12:48.793607Z"}},"outputs":[{"name":"stderr","text":"2025-06-13 01:12:48,791 - INFO - Path dataset ditemukan.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- 1. AverageMeter: Untuk melacak metrik ---\nclass AverageMeter:\n    def __init__(self): self.reset()\n    def reset(self): self.val=0; self.avg=0; self.sum=0; self.count=0\n    def step(self, val, n=1): self.val=val; self.sum+=val*n; self.count+=n; self.avg=self.sum/self.count\n    def get(self): return self.avg\n\n# --- 2. ACC: Fungsi Akurasi ---\ndef ACC(logit, target):\n    pred = torch.argmax(logit, dim=1)\n    correct = (pred == target).sum().item()\n    return correct / target.size(0) * 100\n\n# --- 3. load_state: Memuat bobot dari checkpoint ---\ndef load_state(net, ckpt_state_dict):\n    sd = net.state_dict()\n    nd = {}\n    for k_ckpt, v_ckpt in ckpt_state_dict.items():\n        k_model = k_ckpt.replace('module.', '')\n        if k_model in sd and sd[k_model].shape == v_ckpt.shape:\n            nd[k_model] = v_ckpt\n    if not nd:\n        logging.warning(\"Tidak ada kunci yang cocok dari checkpoint yang dimuat.\")\n        return False\n    missing, unexpected = net.load_state_dict(nd, strict=False)\n    logging.info(f\"Berhasil memuat {len(nd)} kunci. Hilang: {len(missing)}. Tak terduga: {len(unexpected)}\")\n    return True\n\n# --- 4. DeepfakeDataset: Kelas Dataset Kustom ---\nclass DeepfakeDataset(Dataset):\n    def __init__(self, phase, images_dir, labels_file_path, image_size, augment=False):\n        self.phase = phase\n        self.augment = augment\n        self.image_size = image_size\n        \n        # Baca file CSV\n        df_full = pd.read_csv(labels_file_path)\n        # Tentukan path dasar dari mana path di CSV dihitung\n        self.image_paths = [os.path.join(images_dir, rel_path) for rel_path in df_full['filepath']]\n        self.labels = df_full['label'].values\n\n        # --- Split Data Train/Test yang lebih baik (berbasis video) ---\n        # Asumsi format 'c40_real_original_000_0' -> video_id = 'c40_real_original_000'\n        video_ids = [\"_\".join(os.path.basename(p).split('_')[:-1]) for p in self.image_paths]\n        unique_videos = sorted(list(set(video_ids)))\n        random.shuffle(unique_videos)\n        \n        train_split = int(0.8 * len(unique_videos))\n        train_videos = unique_videos[:train_split]\n        \n        train_indices = [i for i, vid in enumerate(video_ids) if vid in train_videos]\n        test_indices = [i for i, vid in enumerate(video_ids) if vid not in train_videos]\n\n        if self.phase == 'train':\n            self.indices = train_indices\n        else: # 'test' atau 'valid'\n            self.indices = test_indices\n            \n        logging.info(f\"Dataset phase: {self.phase}, augment: {self.augment}, #samples: {len(self.indices)}\")\n\n        # --- Transformasi Gambar ---\n        self.transform_train = transforms.Compose([\n            transforms.RandomResizedCrop(self.image_size, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        self.transform_val = transforms.Compose([\n            transforms.Resize(self.image_size),\n            transforms.CenterCrop(self.image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.indices[idx]\n        img_path = self.image_paths[real_idx]\n        label = self.labels[real_idx]\n        \n        try:\n            image = Image.open(img_path).convert('RGB')\n        except FileNotFoundError:\n            logging.error(f\"Gambar tidak ditemukan: {img_path}, menggunakan gambar dummy.\")\n            image = Image.new('RGB', self.image_size, color='red')\n        \n        if self.phase == 'train' and self.augment:\n            image_tensor = self.transform_train(image)\n        else:\n            image_tensor = self.transform_val(image)\n            \n        return image_tensor, torch.tensor(label, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.795155Z","iopub.execute_input":"2025-06-13T01:12:48.795677Z","iopub.status.idle":"2025-06-13T01:12:48.808155Z","shell.execute_reply.started":"2025-06-13T01:12:48.795653Z","shell.execute_reply":"2025-06-13T01:12:48.807536Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Fungsi Training dan Validasi","metadata":{}},{"cell_type":"code","source":"# Cell 5: Fungsi Training dan Validasi per Epoch\n\ndef calculate_combined_loss(loss_pack, config: Config):\n    \"\"\"Menghitung total loss dari berbagai komponen.\"\"\"\n    device = next(iter(loss_pack.values())).device\n    \n    ensemble_loss = loss_pack.get('ensemble_loss', torch.tensor(0.0, device=device))\n    aux_loss = loss_pack.get('aux_loss', torch.tensor(0.0, device=device))\n    agda_loss = loss_pack.get('AGDA_ensemble_loss', torch.tensor(0.0, device=device))\n    match_loss = loss_pack.get('match_loss', torch.tensor(0.0, device=device))\n    \n    total_loss = (config.ensemble_loss_weight * ensemble_loss +\n                  config.aux_loss_weight * aux_loss +\n                  config.AGDA_loss_weight * agda_loss +\n                  config.match_loss_weight * match_loss)\n    return total_loss\n\ndef run_epoch(logs, data_loader, net, optimizer, device, config_obj: Config, agda_module, phase='train'):\n    \"\"\"Menjalankan satu epoch penuh untuk training atau validasi.\"\"\"\n    logging.info(f\"==> Memulai Epoch {logs.get('epoch', '?')} | Fase: {phase.upper()} <==\")\n    \n    is_train = phase == 'train'\n    use_agda = agda_module is not None and config_obj.AGDA_loss_weight > 0 and is_train\n    \n    # Inisialisasi perekam metrik\n    metrics = {\n        'loss': AverageMeter(),\n        'acc': AverageMeter(),\n        'aux_loss': AverageMeter(),\n        'agda_loss': AverageMeter(),\n        'match_loss': AverageMeter()\n    }\n    \n    net.train(is_train)\n    epoch_start_time = time.time()\n    \n    for i, (X_batch, y_batch) in enumerate(data_loader):\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        # --- FORWARD PASS ---\n        with torch.set_grad_enabled(is_train):\n            if is_train:\n                # Panggil metode training model yang mengembalikan dictionary loss\n                loss_pack = net.train_batch(X_batch, y_batch, jump_aux=(config_obj.aux_loss_weight == 0))\n                total_loss = calculate_combined_loss(loss_pack, config_obj)\n            else:\n                # Panggil forward standar untuk validasi yang mengembalikan logits\n                logits = net(X_batch)\n                loss_pack = {'ensemble_logit': logits, 'ensemble_loss': F.cross_entropy(logits, y_batch)}\n                total_loss = loss_pack['ensemble_loss']\n\n        # --- BACKWARD PASS (hanya saat training) ---\n        if is_train:\n            if torch.isnan(total_loss) or torch.isinf(total_loss):\n                logging.error(f\"Loss NaN/Inf terdeteksi pada batch {i}. Melewati backward.\")\n                continue\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n            \n        # --- CATAT METRIK ---\n        with torch.no_grad():\n            metrics['loss'].step(total_loss.item(), X_batch.size(0))\n            metrics['acc'].step(ACC(loss_pack['ensemble_logit'], y_batch), X_batch.size(0))\n            if is_train:\n                metrics['aux_loss'].step(loss_pack.get('aux_loss', 0), X_batch.size(0))\n                metrics['agda_loss'].step(loss_pack.get('AGDA_ensemble_loss', 0), X_batch.size(0))\n                metrics['match_loss'].step(loss_pack.get('match_loss', 0), X_batch.size(0))\n\n        if (i + 1) % 50 == 0 or i == len(data_loader) - 1:\n            logging.info(f\"  [{i+1}/{len(data_loader)}] -> Loss: {metrics['loss'].avg:.4f} | Acc: {metrics['acc'].avg:.2f}%\")\n            \n    # --- RINGKASAN EPOCH (MODIFIKASI) ---\n    epoch_duration = time.time() - epoch_start_time\n    \n    # Gunakan kunci yang unik untuk setiap epoch dan fase\n    current_epoch = logs.get('epoch', '?')\n    log_key_loss = f'epoch_{current_epoch}_{phase}_loss'\n    log_key_acc = f'epoch_{current_epoch}_{phase}_acc'\n    log_key_aux = f'epoch_{current_epoch}_{phase}_aux_loss'\n    \n    # Simpan metrik ke dalam logs\n    logs[log_key_loss] = metrics['loss'].get()\n    logs[log_key_acc] = metrics['acc'].get()\n    \n    summary_str = (\n        f\"==> Fase {phase.upper()} Selesai | Waktu: {epoch_duration:.2f}s | \"\n        f\"Avg Loss: {logs[log_key_loss]:.4f} | \"\n        f\"Avg Acc: {logs[log_key_acc]:.2f}%\"\n    )\n    \n    if is_train:\n        logs[log_key_aux] = metrics['aux_loss'].get()\n        summary_str += f\" | Avg Aux Loss: {logs[log_key_aux]:.4f}\"\n    \n    logging.info(summary_str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.808994Z","iopub.execute_input":"2025-06-13T01:12:48.809328Z","iopub.status.idle":"2025-06-13T01:12:48.825775Z","shell.execute_reply.started":"2025-06-13T01:12:48.809301Z","shell.execute_reply":"2025-06-13T01:12:48.825168Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Pipeline training","metadata":{}},{"cell_type":"code","source":"def train_pipeline(config_obj: Config):\n    \"\"\"Mengatur dan menjalankan seluruh pipeline pelatihan.\"\"\"\n    \n    # --- 1. Persiapan Dataset & DataLoader ---\n    logging.info(\"Mempersiapkan DataLoaders...\")\n    train_dataset = DeepfakeDataset('train', config_obj.images_folder_path, config_obj.labels_csv_path, config_obj.net_config['size'], augment=True)\n    val_dataset = DeepfakeDataset('valid', config_obj.images_folder_path, config_obj.labels_csv_path, config_obj.net_config['size'], augment=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=config_obj.batch_size, shuffle=True, num_workers=config_obj.workers, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=config_obj.batch_size, shuffle=False, num_workers=config_obj.workers, pin_memory=True)\n    logging.info(\"DataLoaders siap.\")\n\n    # --- 2. Inisialisasi Model, Optimizer, Scheduler ---\n    logging.info(f\"Menginisialisasi model MAT dengan backbone: {config_obj.net_config['net']}\")\n    net = MAT(**config_obj.net_config).to(DEVICE)\n    agda_module = AGDA(**config_obj.AGDA_config).to(DEVICE)\n    \n    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=config_obj.learning_rate, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config_obj.scheduler_step, gamma=config_obj.scheduler_gamma)\n    \n    # --- 3. Melanjutkan dari Checkpoint (jika ada) ---\n    start_epoch = 0\n    logs = {'best_val_acc': 0.0}\n    if config_obj.resume_checkpoint and os.path.exists(config_obj.resume_checkpoint):\n        logging.info(f\"Memuat checkpoint dari: {config_obj.resume_checkpoint}\")\n        ckpt = torch.load(config_obj.resume_checkpoint, map_location=DEVICE)\n        load_state(net, ckpt['state_dict'])\n        optimizer.load_state_dict(ckpt['optimizer_state'])\n        scheduler.load_state_dict(ckpt['scheduler_state'])\n        start_epoch = ckpt['logs']['epoch'] + 1\n        logs = ckpt['logs']\n        logging.info(f\"Berhasil memuat. Melanjutkan dari epoch {start_epoch}.\")\n    \n    # --- 4. Loop Pelatihan Utama ---\n    logging.info(f\"Memulai pelatihan dari epoch {start_epoch} hingga {config_obj.epochs-1}.\")\n    for epoch in range(start_epoch, config_obj.epochs):\n        logs['epoch'] = epoch\n        \n        # Fase Training\n        run_epoch(logs, train_loader, net, optimizer, DEVICE, config_obj, agda_module, 'train')\n        \n        # Fase Validasi\n        run_epoch(logs, val_loader, net, optimizer, DEVICE, config_obj, agda_module, 'valid')\n\n        scheduler.step()\n        logging.info(f\"LR diupdate menjadi: {scheduler.get_last_lr()[0]:.2e}\")\n\n        # --- 5. Simpan Checkpoint ---\n        # Bentuk kunci log yang benar untuk akurasi validasi epoch saat ini\n        current_val_acc_key = f'epoch_{epoch}_valid_acc'\n        current_val_acc = logs.get(current_val_acc_key, 0.0) # Dapatkan akurasi saat ini\n        \n        # Bandingkan dengan akurasi terbaik yang pernah disimpan\n        is_best = current_val_acc > logs.get('best_val_acc', 0.0)\n        if is_best:\n            # Update akurasi terbaik di log\n            logs['best_val_acc'] = current_val_acc\n            best_ckpt_path = os.path.join(config_obj.checkpoint_dir, 'ckpt_best.pth')\n            logging.info(f\"Akurasi validasi terbaik baru: {logs['best_val_acc']:.2f}%. Menyimpan ke {best_ckpt_path}\")\n            # Simpan hanya state_dict dan logs untuk model terbaik agar file lebih kecil\n            torch.save({'logs': logs, 'state_dict': net.state_dict()}, best_ckpt_path)\n            \n        # Selalu simpan checkpoint terakhir yang berisi semua state (model, optimizer, scheduler)\n        latest_ckpt_path = os.path.join(config_obj.checkpoint_dir, 'ckpt_latest.pth')\n        torch.save({\n            'logs': logs,\n            'state_dict': net.state_dict(),\n            'optimizer_state': optimizer.state_dict(),\n            'scheduler_state': scheduler.state_dict()\n        }, latest_ckpt_path)\n\n    logging.info(f\"Pelatihan selesai. Akurasi validasi terbaik: {logs['best_val_acc']:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.826439Z","iopub.execute_input":"2025-06-13T01:12:48.826650Z","iopub.status.idle":"2025-06-13T01:12:48.848326Z","shell.execute_reply.started":"2025-06-13T01:12:48.826628Z","shell.execute_reply":"2025-06-13T01:12:48.847595Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"try:\n    train_pipeline(config)\nexcept Exception as e:\n    logging.error(\"Terjadi error fatal saat menjalankan pipeline pelatihan.\", exc_info=True)\nfinally:\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    logging.info(\"Proses selesai atau dihentikan.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T01:12:48.849199Z","iopub.execute_input":"2025-06-13T01:12:48.849377Z"}},"outputs":[{"name":"stderr","text":"2025-06-13 01:12:48,863 - INFO - Mempersiapkan DataLoaders...\n2025-06-13 01:12:49,485 - INFO - Dataset phase: train, augment: True, #samples: 5579\n2025-06-13 01:12:50,104 - INFO - Dataset phase: valid, augment: False, #samples: 1395\n2025-06-13 01:12:50,106 - INFO - DataLoaders siap.\n2025-06-13 01:12:50,106 - INFO - Menginisialisasi model MAT dengan backbone: resnet101\n2025-06-13 01:12:50,107 - INFO - MAT: Initializing backbone 'resnet101'...\n2025-06-13 01:12:50,735 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet101.a1h_in1k)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3814f637d72c4fe4b14aaa2903d7314c"}},"metadata":{}},{"name":"stderr","text":"2025-06-13 01:12:51,579 - INFO - [timm/resnet101.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n2025-06-13 01:12:51,633 - INFO - Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n2025-06-13 01:12:51,641 - INFO - MAT: Dry running backbone to get feature shapes...\n2025-06-13 01:12:51,806 - INFO -   > Feature layer 's2' channels: 256\n2025-06-13 01:12:51,806 - INFO -   > Attention layer 's3' channels: 512\n2025-06-13 01:12:51,807 - INFO -   > Global branch layer 'final_conv' channels: 1024\n2025-06-13 01:12:51,822 - INFO - MAT: Using TextureEnhance v2\n2025-06-13 01:12:51,966 - INFO - MAT: Using AuxiliaryLoss v1\n2025-06-13 01:12:52,165 - INFO - Memulai pelatihan dari epoch 0 hingga 2.\n2025-06-13 01:12:52,166 - INFO - ==> Memulai Epoch 0 | Fase: TRAIN <==\n2025-06-13 01:13:35,717 - INFO -   [50/349] -> Loss: 0.5667 | Acc: 80.62%\n2025-06-13 01:14:17,566 - INFO -   [100/349] -> Loss: 0.5103 | Acc: 83.06%\n2025-06-13 01:14:59,421 - INFO -   [150/349] -> Loss: 0.4880 | Acc: 83.71%\n2025-06-13 01:15:41,272 - INFO -   [200/349] -> Loss: 0.4709 | Acc: 84.44%\n2025-06-13 01:16:23,152 - INFO -   [250/349] -> Loss: 0.4573 | Acc: 84.85%\n2025-06-13 01:17:05,054 - INFO -   [300/349] -> Loss: 0.4426 | Acc: 85.33%\n2025-06-13 01:17:46,396 - INFO -   [349/349] -> Loss: 0.4477 | Acc: 84.71%\n2025-06-13 01:17:46,440 - INFO - ==> Fase TRAIN Selesai | Waktu: 294.27s | Avg Loss: 0.4477 | Avg Acc: 84.71% | Avg Aux Loss: 0.3922\n2025-06-13 01:17:46,442 - INFO - ==> Memulai Epoch 0 | Fase: VALID <==\n2025-06-13 01:17:55,357 - INFO -   [50/88] -> Loss: 0.4334 | Acc: 77.75%\n2025-06-13 01:18:02,018 - INFO -   [88/88] -> Loss: 0.3580 | Acc: 86.95%\n2025-06-13 01:18:02,053 - INFO - ==> Fase VALID Selesai | Waktu: 15.61s | Avg Loss: 0.3580 | Avg Acc: 86.95%\n2025-06-13 01:18:02,055 - INFO - LR diupdate menjadi: 9.50e-05\n2025-06-13 01:18:02,056 - INFO - Akurasi validasi terbaik baru: 86.95%. Menyimpan ke /kaggle/working/checkpoints/ffpp_mat_resnet101_v1/ckpt_best.pth\n2025-06-13 01:18:03,403 - INFO - ==> Memulai Epoch 1 | Fase: TRAIN <==\n2025-06-13 01:18:45,510 - INFO -   [50/349] -> Loss: 0.4164 | Acc: 85.25%\n2025-06-13 01:19:27,399 - INFO -   [100/349] -> Loss: 0.4120 | Acc: 85.00%\n2025-06-13 01:20:09,267 - INFO -   [150/349] -> Loss: 0.4049 | Acc: 84.71%\n2025-06-13 01:20:51,125 - INFO -   [200/349] -> Loss: 0.3958 | Acc: 85.22%\n2025-06-13 01:21:32,985 - INFO -   [250/349] -> Loss: 0.3964 | Acc: 85.08%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# ===================================================================\n# 1. Grafik Training & Validation Loss dan Accuracy\n# ===================================================================\n\ndef plot_training_history(logs):\n    \"\"\"Membuat plot untuk loss dan akurasi selama training.\"\"\"\n    \n    epochs = range(logs['epoch'] + 1)\n    \n    train_loss = [logs.get(f'epoch_{e}_train_loss', None) for e in epochs]\n    val_loss = [logs.get(f'epoch_{e}_valid_loss', None) for e in epochs]\n    \n    train_acc = [logs.get(f'epoch_{e}_train_acc', None) for e in epochs]\n    val_acc = [logs.get(f'epoch_{e}_valid_acc', None) for e in epochs]\n\n    # Membersihkan nilai None jika training dihentikan di tengah jalan\n    epochs_loss = [e for e, tl, vl in zip(epochs, train_loss, val_loss) if tl is not None and vl is not None]\n    train_loss = [tl for tl in train_loss if tl is not None]\n    val_loss = [vl for vl in val_loss if vl is not None]\n    \n    epochs_acc = [e for e, ta, va in zip(epochs, train_acc, val_acc) if ta is not None and va is not None]\n    train_acc = [ta for ta in train_acc if ta is not None]\n    val_acc = [va for va in val_acc if va is not None]\n    \n    plt.style.use('seaborn-v0_8-whitegrid')\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n    \n    # Plot Loss\n    ax1.plot(epochs_loss, train_loss, 'o-', label='Training Loss')\n    ax1.plot(epochs_loss, val_loss, 'o-', label='Validation Loss')\n    ax1.set_title('Training and Validation Loss', fontsize=16)\n    ax1.set_xlabel('Epoch', fontsize=12)\n    ax1.set_ylabel('Loss', fontsize=12)\n    ax1.legend()\n    ax1.set_xticks(epochs_loss)\n\n    # Plot Accuracy\n    ax2.plot(epochs_acc, train_acc, 'o-', label='Training Accuracy')\n    ax2.plot(epochs_acc, val_acc, 'o-', label='Validation Accuracy')\n    ax2.set_title('Training and Validation Accuracy', fontsize=16)\n    ax2.set_xlabel('Epoch', fontsize=12)\n    ax2.set_ylabel('Accuracy (%)', fontsize=12)\n    ax2.legend()\n    ax2.set_xticks(epochs_acc)\n    \n    fig.suptitle('Training History', fontsize=20)\n    plt.show()\n\ntry:\n    if 'logs' in locals() and logs:\n        print(\"Membuat plot dari log training yang ada di memori...\")\n        plot_training_history(logs)\n    else:\n        raise NameError(\"Variabel 'logs' tidak ditemukan.\")\n# Jika gagal (misal, kernel di-restart), coba muat dari checkpoint\nexcept (NameError, KeyError):\n    print(\"Log dari memori tidak ditemukan. Mencoba memuat dari checkpoint terakhir...\")\n    \n    # Pastikan variabel `config` ada\n    if 'config' in locals():\n        latest_ckpt_path = os.path.join(config.checkpoint_dir, 'ckpt_latest.pth')\n        \n        if os.path.exists(latest_ckpt_path):\n            ckpt = torch.load(latest_ckpt_path, map_location='cpu') # Muat ke CPU agar tidak butuh GPU\n            logs_from_ckpt = ckpt.get('logs')\n            \n            if logs_from_ckpt:\n                print(\"Berhasil memuat log dari checkpoint. Membuat plot...\")\n                plot_training_history(logs_from_ckpt)\n            else:\n                print(\"Checkpoint ditemukan, tetapi tidak berisi 'logs'.\")\n        else:\n            print(f\"Checkpoint terakhir tidak ditemukan di: {latest_ckpt_path}\")\n    else:\n        print(\"Variabel 'config' tidak ditemukan untuk mencari path checkpoint.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# 2. Confusion Matrix dan Laporan Klasifikasi\n# ===================================================================\n\ndef evaluate_and_plot_confusion_matrix(config_obj: Config):\n    \"\"\"Mengevaluasi model pada set validasi dan membuat confusion matrix.\"\"\"\n    \n    # --- Muat model terbaik ---\n    best_model_path = os.path.join(config_obj.checkpoint_dir, 'ckpt_best.pth')\n    if not os.path.exists(best_model_path):\n        print(f\"Model terbaik tidak ditemukan di {best_model_path}. Menggunakan model terakhir.\")\n        best_model_path = os.path.join(config_obj.checkpoint_dir, 'ckpt_latest.pth')\n        if not os.path.exists(best_model_path):\n            print(\"Tidak ada model yang bisa dimuat untuk evaluasi.\")\n            return\n\n    print(f\"Memuat model dari: {best_model_path}\")\n    net = MAT(**config_obj.net_config).to(DEVICE)\n    ckpt = torch.load(best_model_path, map_location=DEVICE)\n    load_state(net, ckpt['state_dict'])\n    net.eval()\n    \n    # --- Siapkan DataLoader untuk validasi ---\n    val_dataset = DeepfakeDataset('valid', config_obj.images_folder_path, config_obj.labels_csv_path, config_obj.net_config['size'], augment=False)\n    val_loader = DataLoader(val_dataset, batch_size=config_obj.batch_size, shuffle=False, num_workers=config_obj.workers, pin_memory=True)\n    \n    all_preds = []\n    all_labels = []\n    \n    print(\"Menjalankan evaluasi pada set validasi...\")\n    with torch.no_grad():\n        for i, (X_batch, y_batch) in enumerate(val_loader):\n            X_batch = X_batch.to(DEVICE)\n            \n            logits = net(X_batch)\n            preds = torch.argmax(logits, dim=1)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(y_batch.cpu().numpy())\n            \n            if (i+1) % 50 == 0:\n                print(f\"  Batch {i+1}/{len(val_loader)} selesai.\")\n    \n    print(\"Evaluasi selesai.\")\n\n    # --- Tampilkan Classification Report ---\n    class_names = ['REAL', 'FAKE'] # Sesuaikan jika label Anda berbeda\n    print(\"\\n\" + \"=\"*50)\n    print(\"Classification Report\")\n    print(\"=\"*50)\n    print(classification_report(all_labels, all_preds, target_names=class_names))\n\n    # --- Plot Confusion Matrix ---\n    cm = confusion_matrix(all_labels, all_preds)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix', fontsize=16)\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.show()\n\nevaluate_and_plot_confusion_matrix(config)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}